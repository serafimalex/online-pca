{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d907e0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tools.oja_pca import OjaPCA\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from online_svd_buffer import fit_batched\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "# repo_root = Path(r\"D:\\PyParSVD\")\n",
    "# assert (repo_root / \"pyparsvd\").is_dir(), \"pyparsvd/ not found under D:\\\\PyParSVD\"\n",
    "\n",
    "# # Put repo root (the parent of 'pyparsvd/') on sys.path\n",
    "# sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# from pyparsvd.parsvd_serial   import ParSVD_Serial\n",
    "# from pyparsvd.parsvd_parallel import ParSVD_Parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c0a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_subset(n_samples=5000):\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X = mnist.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "\n",
    "    return X  # shape: (784, n_samples)\n",
    "\n",
    "def load_fashion_mnist_subset(n_samples=5000):\n",
    "    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "    X = fmnist.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (784, n_samples)\n",
    "\n",
    "def load_usps_subset(n_samples=5000):\n",
    "    usps = fetch_openml('USPS', version=1, as_frame=False)\n",
    "    X = usps.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "\n",
    "    return X  # shape: (256, n_samples) since USPS has 16x16 images\n",
    "\n",
    "def load_isolet_subset(n_samples=5000):\n",
    "    isolet = fetch_openml('isolet', version=1, as_frame=False)\n",
    "    # Features are already real-valued, just normalize by max\n",
    "    X = isolet.data.astype(np.float32).T[:, :n_samples]\n",
    "    print(X.shape)\n",
    "    X /= np.max(X)  # scale to [0,1]\n",
    "    return X  # shape: (617, n_samples), 617 audio features\n",
    "\n",
    "def load_mnist_and_fashion(n_samples=5000):\n",
    "    # Load MNIST\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X_mnist = mnist.data.astype(np.float32)[:n_samples] / 255.0\n",
    "    y_mnist = mnist.target.astype(int)[:n_samples]\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "    X_fmnist = fmnist.data.astype(np.float32)[:n_samples] / 255.0\n",
    "    y_fmnist = fmnist.target.astype(int)[:n_samples]\n",
    "\n",
    "    # Stack them together\n",
    "    X = np.vstack([X_mnist, X_fmnist]).T   # shape: (784, 2*n_samples)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09859e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_isolet_combined(n_samples=5000):\n",
    "    X_mnist = load_mnist_subset(n_samples).astype(np.float32, copy=False)   # (784, n)\n",
    "    X_iso   = load_isolet_subset(n_samples).astype(np.float32, copy=False)  # (617, n)\n",
    "\n",
    "    d_mnist, _ = X_mnist.shape\n",
    "    d_iso,   _ = X_iso.shape\n",
    "    d_common = min(d_mnist, d_iso)\n",
    "\n",
    "    if d_mnist > d_common:\n",
    "        X_mnist = X_mnist[:d_common, :]\n",
    "    if d_iso > d_common:\n",
    "        X_iso = X_iso[:d_common, :]\n",
    "\n",
    "    X_combined = np.concatenate([X_mnist, X_iso], axis=1).astype(np.float32, copy=False)\n",
    "\n",
    "    mu = X_combined.mean(axis=1, keepdims=True).astype(X_combined.dtype)\n",
    "    X_combined = X_combined - mu\n",
    "\n",
    "    print(X_combined.shape)  # (d_common, 2*n_samples)\n",
    "    return X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a649f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "def explained_variance_ratio(X, X_recon):\n",
    "    error = np.linalg.norm(X - X_recon, 'fro') ** 2\n",
    "    total = np.linalg.norm(X, 'fro') ** 2\n",
    "    return 1 - error / total\n",
    "\n",
    "def plot_traces(traces, X, p):\n",
    "\n",
    "    traces = np.array(traces)\n",
    "    u, s, vt = np.linalg.svd(X)\n",
    "    aux = u.transpose() @ X @ vt.transpose()\n",
    "    true_energy = np.trace(aux[:p, :p])\n",
    "\n",
    "\n",
    "    true_energy = np.sum(np.linalg.svd(X, compute_uv=False)[:p])\n",
    "\n",
    "    print(true_energy)\n",
    "    print(traces[:-20])\n",
    "    \n",
    "    plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlepad\": 8,\n",
    "    })\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.0, 4.2))\n",
    "\n",
    "    # Data\n",
    "    ax.plot(traces, label='OnlinePCA', linewidth=2)\n",
    "    ax.axhline(true_energy, linestyle=':', linewidth=2,\n",
    "            label='Sum of first p singular values')\n",
    "\n",
    "    # Y-limit: 5% above the red line (and safe if traces ever exceed it)\n",
    "    y_top = 1.10 * max(true_energy, np.nanmax(traces))\n",
    "    y_bottom = min(0, float(np.nanmin(traces)))\n",
    "    ax.set_ylim(y_bottom, y_top)\n",
    "\n",
    "    # Clean up axes\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    ax.grid(axis='y', linestyle='--', linewidth=0.6, alpha=0.4)\n",
    "    ax.margins(x=0)  # no extra horizontal padding\n",
    "\n",
    "    # Tick formatting: show iterations as “k”\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, pos: f'{int(x/1000)}k' if x >= 1000 else f'{int(x)}')\n",
    "    )\n",
    "\n",
    "    # Labels & legend\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Trace')\n",
    "    ax.set_title('Trace progression vs. True energy (random)')\n",
    "    ax.legend(frameon=False, loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('trace_mnist.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabe223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(method_name, fit_fn):\n",
    "    start = time.time()\n",
    "    U, S, Vt, X_recon, traces = fit_fn()\n",
    "    elapsed = time.time() - start\n",
    "    # if traces is not None:\n",
    "    #     plot_traces(traces, X, p)\n",
    "    evr = explained_variance_ratio(X, X_recon)\n",
    "    \n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"time\": elapsed,\n",
    "        \"explained_variance\": evr,\n",
    "        \"traces\": traces\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b3cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "# from online_psp.online_psp.fast_similarity_matching import FSM\n",
    "# from online_psp.online_psp.similarity_matching import SM\n",
    "# from online_psp.online_psp.incremental_pca import IPCA\n",
    "\n",
    "def run_benchmarks(X, p=50, g=200):\n",
    "    results = []\n",
    "\n",
    "    #sklearn PCA (full SVD)\n",
    "    def run_pca():\n",
    "        model = PCA(n_components=p, svd_solver=\"full\")\n",
    "        model.fit(X.T)\n",
    "        X_recon = model.inverse_transform(model.transform(X.T)).T\n",
    "        print(\"Done PCA\")\n",
    "        return model.components_.T, model.singular_values_, None, X_recon, None\n",
    "    results.append(benchmark(\"PCA\", run_pca))\n",
    "\n",
    "    def run_online_pca():\n",
    "        batch_sz = X.shape[0] * 2\n",
    "        traces_raw, U, X_approx = fit_batched(X, p, g, batch_sz)  \n",
    "        traces = []\n",
    "\n",
    "        total_var = np.var(X, ddof=0)\n",
    "\n",
    "        for X_recon_batch in traces_raw:\n",
    "            batch_var = np.var(X - X_recon_batch, ddof=0)\n",
    "            evr = 1.0 - batch_var / total_var\n",
    "            traces.append(evr)\n",
    "\n",
    "        X_reduced = U.T[:p, :] @ X\n",
    "        X_recon = U[:, :p] @ X_reduced\n",
    "\n",
    "        return U, None, None, X_recon, np.array(traces)\n",
    "\n",
    "    \n",
    "    results.append(benchmark(\"OnlinePCA\", run_online_pca))\n",
    "\n",
    "    # Incremental PCA\n",
    "    def run_incpca():\n",
    "        batch_sz = X.shape[0] * 2\n",
    "        model = IncrementalPCA(n_components=p, batch_size=batch_sz)\n",
    "        \n",
    "        X_batches = np.array_split(X.T, max(1, X.shape[1] // batch_sz), axis=0)\n",
    "        \n",
    "        explained_variances = []\n",
    "        \n",
    "        for batch in X_batches:\n",
    "            model.partial_fit(batch)\n",
    "            \n",
    "            X_recon = model.inverse_transform(model.transform(X.T)).T\n",
    "            \n",
    "            evr = explained_variance_ratio(X, X_recon) \n",
    "            explained_variances.append(evr)\n",
    "        \n",
    "        X_recon_final = model.inverse_transform(model.transform(X.T)).T\n",
    "        \n",
    "        print(\"Done Incremental PCA with tracked explained variance\")\n",
    "        return model.components_.T, None, None, X_recon_final, np.array(explained_variances)\n",
    "    \n",
    "    results.append(benchmark(\"IncrementalPCA\", run_incpca))\n",
    "\n",
    "    def run_oja():\n",
    "        model = OjaPCA(\n",
    "            n_features=X.shape[0],\n",
    "            n_components=p,\n",
    "            eta=0.001,\n",
    "        )\n",
    "\n",
    "        X_tensor = torch.tensor(X.T, dtype=torch.float32)\n",
    "        b_size = X.shape[0] * 2\n",
    "\n",
    "        explained_variances = []\n",
    "\n",
    "        for i in range(0, len(X_tensor), b_size):\n",
    "            batch = X_tensor[i : i + b_size]\n",
    "\n",
    "            if len(batch) < b_size:\n",
    "                batch = torch.cat([batch, X_tensor[: b_size - len(batch)]], dim=0)\n",
    "\n",
    "            if hasattr(model, \"forward\"):\n",
    "                model(batch)\n",
    "            else:\n",
    "                model.partial_fit(batch)\n",
    "\n",
    "            # after each batch: compute reconstruction + explained variance\n",
    "            with torch.no_grad():\n",
    "                recon = model.inverse_transform(model.transform(X_tensor))\n",
    "                evr = explained_variance_ratio(X, np.array(recon).T)\n",
    "                explained_variances.append(evr)\n",
    "\n",
    "        recon = model.inverse_transform(model.transform(X_tensor))\n",
    "        print(\"Done Oja PCA\")\n",
    "\n",
    "        return (\n",
    "            np.array(model.get_components()),\n",
    "            None,\n",
    "            None,\n",
    "            np.array(recon).T,\n",
    "            np.array(explained_variances),\n",
    "        )\n",
    "    results.append(benchmark(\"OjaPCA\", run_oja))\n",
    "\n",
    "    # def run_ccipca():\n",
    "    #     sigma2_0 = 1e-8 * np.ones(p)\n",
    "    #     Uhat0 = (X[:, :p] / np.sqrt((X[:, :p] ** 2).sum(0))).astype(np.float64)\n",
    "    #     ccipca = CCIPCA(p, X.shape[0], Uhat0=Uhat0, sigma2_0=sigma2_0, cython=True)\n",
    "    #     n_epoch = 1\n",
    "    #     for n_e in range(n_epoch):\n",
    "    #         for x in tqdm(X.T):\n",
    "    #             ccipca.fit_next(x.astype(np.float64))\n",
    "    #     X_reduced = ccipca.get_components().T @ X\n",
    "    #     X_recon = ccipca.get_components() @ X_reduced\n",
    "    #     print(\"Done ccipca\")\n",
    "    #     return np.array(ccipca.get_components()), None, None, np.array(X_recon), None\n",
    "    # results.append(benchmark(\"CCIPCA\", run_ccipca))\n",
    "\n",
    "    # def run_fsm():\n",
    "    #     scal = 100\n",
    "    #     Uhat0 = X[:, :p] / np.sqrt((X[:, :p] ** 2).sum(0)) / scal\n",
    "    #     Minv0    = np.eye(p) * scal\n",
    "\n",
    "\n",
    "    #     errs = []\n",
    "    #     D = X.shape[0]\n",
    "    #     fsm = FSM(p, D, W0=Uhat0.T, Minv0=Minv0)\n",
    "\n",
    "    #     time_1 = time.time()\n",
    "    #     for n_e in range(3):\n",
    "    #         for x in X.T:\n",
    "    #             fsm.fit_next(x)\n",
    "    #     X_reduced = fsm.get_components().T @ X\n",
    "    #     X_recon = fsm.get_components() @ X_reduced\n",
    "    #     print(\"Done FSM\")\n",
    "    #     return np.array(fsm.get_components()), None, None, np.array(X_recon), None\n",
    "    # results.append(benchmark(\"FSM\", run_fsm))\n",
    "\n",
    "    # def run_sm():\n",
    "    #     scal = 100\n",
    "    #     Uhat0 = X[:, :p] / np.sqrt((X[:, :p] ** 2).sum(0)) / scal\n",
    "    #     M0    = np.eye(p) / scal\n",
    "\n",
    "    #     errs = []\n",
    "    #     D = X.shape[0]\n",
    "    #     sm = SM(p, D, W0=Uhat0.T, M0=M0)\n",
    "\n",
    "    #     time_1 = time.time()\n",
    "    #     for n_e in range(5):\n",
    "    #         for x in tqdm(X.T):\n",
    "    #             sm.fit_next(x)\n",
    "    #     X_reduced = sm.get_components().T @ X\n",
    "    #     X_recon = sm.get_components() @ X_reduced\n",
    "    #     print(\"Done SM\")\n",
    "    #     return np.array(sm.get_components()), None, None, np.array(X_recon), None\n",
    "    # results.append(benchmark(\"SimilarityMatching\", run_sm))\n",
    "\n",
    "    # def run_ipca2():\n",
    "    #     sigma2_0 = lambda0 = np.zeros(p)\n",
    "    #     Uhat0 = X[:, :p] / np.sqrt((X[:, :p] ** 2).sum(0))\n",
    "\n",
    "    #     errs = []\n",
    "    #     D = X.shape[0]\n",
    "    #     ipca = IPCA(p, D, Uhat0=Uhat0, sigma2_0=sigma2_0)\n",
    "    #     for n_e in range(1):\n",
    "    #         for x in tqdm(X.T):\n",
    "    #             ipca.fit_next(x)\n",
    "    #     X_reduced = ipca.get_components().T @ X\n",
    "    #     X_recon = ipca.get_components() @ X_reduced\n",
    "    #     print(\"Done IPCA2\")\n",
    "    #     return np.array(ipca.get_components()), None, None, np.array(X_recon), None\n",
    "    # results.append(benchmark(\"StochasticPCA\", run_ipca2))\n",
    "\n",
    "    # def run_parsvd():\n",
    "    #     N = X.shape[1]\n",
    "    #     batch_size = 2 * X.shape[0]  # change as you like\n",
    "    #     ParSVD = ParSVD_Parallel(K=p, ff=1.0, low_rank=True)\n",
    "    #     first = True\n",
    "    #     for start in range(0, N, batch_size):\n",
    "    #         end = min(start + batch_size, N)\n",
    "    #         batch = X[:, start:end]\n",
    "    #         if first:\n",
    "    #             ParSVD.initialize(batch)\n",
    "    #             first = False\n",
    "    #         else:\n",
    "    #             ParSVD.incorporate_data(batch)\n",
    "    #         print(f\"Processed cols [{start}:{end})\")\n",
    "\n",
    "    #     U = ParSVD.modes\n",
    "    #     X_recon = U @ (U.T @ X)\n",
    "    #     return None, None, None, np.array(X_recon), None\n",
    "    # results.append(benchmark(\"ParSVD\", run_parsvd))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8396c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000)\n"
     ]
    }
   ],
   "source": [
    "X = load_mnist_subset(n_samples=50000)\n",
    "#X = np.random.rand(500,10000).astype(np.float32)\n",
    "n_samples = 50000\n",
    "# X = load_usps_subset(n_samples=9298)\n",
    "# n_samples = 9298\n",
    "\n",
    "# X = load_isolet_subset(n_samples=4704)\n",
    "# n_samples = 4704\n",
    "\n",
    "repeats = int(np.ceil(n_samples / X.shape[1]))\n",
    "X = np.tile(X, (1, repeats))[:, :n_samples]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71a9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\master-AI\\online PCA\\online-pca\\online_svd_buffer.py:226: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"online_svd_buffer.py\", line 86:\u001b[0m\n",
      "\u001b[1m@njit(parallel=True)\n",
      "\u001b[1mdef compute_score_cf_numba(i, j, x, d):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  val = compute_score_cf_numba(iq, s, x, d)[2]\n",
      "d:\\master-AI\\online PCA\\online-pca\\online_svd_buffer.py:292: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"online_svd_buffer.py\", line 86:\u001b[0m\n",
      "\u001b[1m@njit(parallel=True)\n",
      "\u001b[1mdef compute_score_cf_numba(i, j, x, d):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  val = compute_score_cf_numba(r, iq, x, d)[2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\master-AI\\online PCA\\online-pca\\online_svd_buffer.py:226: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"online_svd_buffer.py\", line 86:\u001b[0m\n",
      "\u001b[1m@njit(parallel=True)\n",
      "\u001b[1mdef compute_score_cf_numba(i, j, x, d):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  val = compute_score_cf_numba(iq, s, x, d)[2]\n",
      "d:\\master-AI\\online PCA\\online-pca\\online_svd_buffer.py:292: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"online_svd_buffer.py\", line 86:\u001b[0m\n",
      "\u001b[1m@njit(parallel=True)\n",
      "\u001b[1mdef compute_score_cf_numba(i, j, x, d):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  val = compute_score_cf_numba(r, iq, x, d)[2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 1\n",
      "Done batch 2\n",
      "Done batch 3\n",
      "Done batch 4\n",
      "Done batch 5\n",
      "Done batch 6\n",
      "Done batch 7\n",
      "Done batch 8\n",
      "Done batch 9\n",
      "Done batch 10\n",
      "Done batch 11\n",
      "Done batch 12\n",
      "Done batch 13\n",
      "Done batch 14\n",
      "Done batch 15\n",
      "Done batch 16\n",
      "Done batch 17\n",
      "Done batch 18\n",
      "Done batch 19\n",
      "Done batch 20\n",
      "Done batch 21\n",
      "Done batch 22\n",
      "Done batch 23\n",
      "Done batch 24\n",
      "Done batch 25\n",
      "Done batch 26\n",
      "Done batch 27\n",
      "Done batch 28\n",
      "Done batch 29\n",
      "Done batch 30\n",
      "Done batch 31\n"
     ]
    }
   ],
   "source": [
    "temp = X.copy()\n",
    "# numba warmup\n",
    "_, _, _ = fit_batched(temp, 15, 100, 1576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e0727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done PCA\n",
      "Total batches: 32\n",
      "Done batch 0\n",
      "Done batch 1\n",
      "Done batch 2\n",
      "Done batch 3\n",
      "Done batch 4\n",
      "Done batch 5\n",
      "Done batch 6\n",
      "Done batch 7\n",
      "Done batch 8\n",
      "Done batch 9\n",
      "Done batch 10\n",
      "Done batch 11\n",
      "Done batch 12\n",
      "Done batch 13\n",
      "Done batch 14\n",
      "Done batch 15\n",
      "Done batch 16\n",
      "Done batch 17\n",
      "Done batch 18\n",
      "Done batch 19\n",
      "Done batch 20\n",
      "Done batch 21\n",
      "Done batch 22\n",
      "Done batch 23\n",
      "Done batch 24\n",
      "Done batch 25\n",
      "Done batch 26\n",
      "Done batch 27\n",
      "Done batch 28\n",
      "Done batch 29\n",
      "Done batch 30\n",
      "Done batch 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m p = \u001b[32m15\u001b[39m\n\u001b[32m      2\u001b[39m g = \u001b[32m450\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = run_benchmarks(X, p=p, g=g)\n\u001b[32m      5\u001b[39m results.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m d: d[\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m]) \n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBenchmark Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mrun_benchmarks\u001b[39m\u001b[34m(X, p, g)\u001b[39m\n\u001b[32m     32\u001b[39m     X_recon = U[:, :p] @ X_reduced\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m U, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, X_recon, np.array(traces)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m results.append(benchmark(\u001b[33m\"\u001b[39m\u001b[33mOnlinePCA\u001b[39m\u001b[33m\"\u001b[39m, run_online_pca))\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Incremental PCA\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_incpca\u001b[39m():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(method_name, fit_fn)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbenchmark\u001b[39m(method_name, fit_fn):\n\u001b[32m      2\u001b[39m     start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     U, S, Vt, X_recon, traces = fit_fn()\n\u001b[32m      4\u001b[39m     elapsed = time.time() - start\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# if traces is not None:\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#     plot_traces(traces, X, p)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_benchmarks.<locals>.run_online_pca\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m total_var = np.var(X, ddof=\u001b[32m0\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_recon_batch \u001b[38;5;129;01min\u001b[39;00m traces_raw:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     batch_var = np.var(X - X_recon_batch, ddof=\u001b[32m0\u001b[39m)\n\u001b[32m     28\u001b[39m     evr = \u001b[32m1.0\u001b[39m - batch_var / total_var\n\u001b[32m     29\u001b[39m     traces.append(evr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Radu Filipescu\\.conda\\envs\\online_pca\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:4268\u001b[39m, in \u001b[36mvar\u001b[39m\u001b[34m(a, axis, dtype, out, ddof, keepdims, where, mean, correction)\u001b[39m\n\u001b[32m   4265\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m4268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[32m   4269\u001b[39m                      **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Radu Filipescu\\.conda\\envs\\online_pca\\Lib\\site-packages\\numpy\\_core\\_methods.py:194\u001b[39m, in \u001b[36m_var\u001b[39m\u001b[34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[39m\n\u001b[32m    191\u001b[39m x = asanyarray(arr - arrmean)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr.dtype.type, (nt.floating, nt.integer)):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     x = um.multiply(x, x, out=x)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Fast-paths for built-in complex types\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m x.dtype \u001b[38;5;129;01min\u001b[39;00m _complex_to_float:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "p = 15\n",
    "g = 450\n",
    "\n",
    "results = run_benchmarks(X, p=p, g=g)\n",
    "results.sort(key=lambda d: d[\"time\"]) \n",
    "print(\"\\nBenchmark Results:\")\n",
    "for r in results:\n",
    "    print(f\"{r['method']:15s} | Time: {r['time']:.2f}s | Explained Var: {r['explained_variance']*100:.2f}%\")\n",
    "\n",
    "methods = [r[\"method\"] for r in results]\n",
    "times = [r[\"time\"] for r in results]\n",
    "evrs = [r[\"explained_variance\"] for r in results]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.bar(methods, times, alpha=0.6, label=\"Time (s)\")\n",
    "ax2.plot(methods, evrs, \"o-\", color=\"red\", label=\"Explained Var\")\n",
    "\n",
    "ax1.set_ylabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Explained Variance\")\n",
    "plt.title(f\"PCA Benchmark (p={p}, g={g}) on MNIST subset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Colors for each method\n",
    "colors = {\n",
    "    \"OnlinePCA\": \"blue\",\n",
    "    \"OjaPCA\": \"green\",\n",
    "    \"IncrementalPCA\": \"red\",\n",
    "}\n",
    "\n",
    "for r in results:\n",
    "    traces = r.get(\"traces\", None)\n",
    "    if traces is not None:\n",
    "        traces = np.array(traces, dtype=float)\n",
    "        color = colors.get(r[\"method\"], \"black\")\n",
    "        plt.plot(traces, label=r[\"method\"], color=color, lw=2)\n",
    "\n",
    "plt.xlabel(\"Epoch / Batch\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Explained Variance Growth per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online_pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
