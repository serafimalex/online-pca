{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d907e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from oja_pca import OjaPCA\n",
    "import torch\n",
    "\n",
    "from svd import ApproxSVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea448f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fbpca\n",
    "    HAS_FBPCA = True\n",
    "except ImportError:\n",
    "    HAS_FBPCA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c0a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_subset(n_samples=5000):\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X = mnist.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (784, n_samples)\n",
    "\n",
    "def load_fashion_mnist_subset(n_samples=5000):\n",
    "    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "    X = fmnist.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (784, n_samples)\n",
    "\n",
    "def load_usps_subset(n_samples=5000):\n",
    "    usps = fetch_openml('USPS', version=1, as_frame=False)\n",
    "    X = usps.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (256, n_samples) since USPS has 16x16 images\n",
    "\n",
    "def load_isolet_subset(n_samples=5000):\n",
    "    isolet = fetch_openml('isolet', version=1, as_frame=False)\n",
    "    # Features are already real-valued, just normalize by max\n",
    "    X = isolet.data.astype(np.float32).T[:, :n_samples]\n",
    "    X /= np.max(X)  # scale to [0,1]\n",
    "    return X  # shape: (617, n_samples), 617 audio features\n",
    "\n",
    "def load_mnist_and_fashion(n_samples=5000):\n",
    "    # Load MNIST\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X_mnist = mnist.data.astype(np.float32)[:n_samples] / 255.0\n",
    "    y_mnist = mnist.target.astype(int)[:n_samples]\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "    X_fmnist = fmnist.data.astype(np.float32)[:n_samples] / 255.0\n",
    "    y_fmnist = fmnist.target.astype(int)[:n_samples]\n",
    "\n",
    "    # Stack them together\n",
    "    X = np.vstack([X_mnist, X_fmnist]).T   # shape: (784, 2*n_samples)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a649f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance_ratio(X, X_recon):\n",
    "    print(X.shape)\n",
    "    print(X_recon.shape)\n",
    "    error = np.linalg.norm(X - X_recon, 'fro') ** 2\n",
    "    total = np.linalg.norm(X, 'fro') ** 2\n",
    "    return 1 - error / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabe223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(method_name, fit_fn):\n",
    "    start = time.time()\n",
    "    U, S, Vt, X_recon = fit_fn()\n",
    "    elapsed = time.time() - start\n",
    "    evr = explained_variance_ratio(X, X_recon)\n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"time\": elapsed,\n",
    "        \"explained_variance\": evr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b3cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmarks(X, p=50, g=200):\n",
    "    results = []\n",
    "\n",
    "    ApproxSVD\n",
    "    def run_approx():\n",
    "        approx_svd = ApproxSVD(n_iter=g, p=p,\n",
    "                               score_method=\"cf\",\n",
    "                               debug_mode=True,\n",
    "                               jobs=8,\n",
    "                               stored_g = False,\n",
    "                               use_shared_memory=False,\n",
    "                               use_heap=\"optimized_heap\")\n",
    "        _, U, X_approx = approx_svd.fit_batched(X, 100000)\n",
    "        X_reduced = U.T[:p, :] @ X\n",
    "        X_recon = U[:, :p] @ X_reduced\n",
    "        return U, None, None, X_recon\n",
    "    results.append(benchmark(\"ApproxSVD\", run_approx))\n",
    "\n",
    "    # sklearn PCA (full SVD)\n",
    "    def run_pca():\n",
    "        model = PCA(n_components=p, svd_solver=\"full\")\n",
    "        model.fit(X.T)\n",
    "        X_recon = model.inverse_transform(model.transform(X.T)).T\n",
    "        return model.components_.T, model.singular_values_, None, X_recon\n",
    "    results.append(benchmark(\"PCA (full)\", run_pca))\n",
    "\n",
    "    # Incremental PCA\n",
    "    def run_incpca():\n",
    "        model = IncrementalPCA(n_components=p, batch_size=100000)\n",
    "        model.fit(X.T)\n",
    "        X_recon = model.inverse_transform(model.transform(X.T)).T\n",
    "        return model.components_.T, None, None, X_recon\n",
    "    results.append(benchmark(\"IncrementalPCA\", run_incpca))\n",
    "\n",
    "     #TruncatedSVD (randomized)\n",
    "    def run_tsvd():\n",
    "        model = TruncatedSVD(n_components=p)\n",
    "        X_reduced = model.fit_transform(X.T)\n",
    "        # Reconstruction: approximate, since TSVD doesn't store mean\n",
    "        X_recon = (X_reduced @ model.components_).T\n",
    "        return model.components_.T, None, None, X_recon\n",
    "    results.append(benchmark(\"TruncatedSVD\", run_tsvd))\n",
    "\n",
    "    def run_oja():\n",
    "        model = OjaPCA(\n",
    "            n_features=X.shape[0],\n",
    "            n_components=p,\n",
    "            eta=0.005,\n",
    "        )\n",
    "        X_tensor = torch.tensor(X.T)\n",
    "        b_size = 100000\n",
    "        for i in range(0, len(X_tensor) - b_size, b_size):\n",
    "            batch = X_tensor[i : i + b_size]\n",
    "            if len(batch) < b_size:\n",
    "                # This line means we use up to an extra partial batch over 1 pass\n",
    "                batch = torch.cat([batch, X_tensor[: b_size - len(batch)]], dim=0)\n",
    "            error = model(batch) if hasattr(model, \"forward\") else None\n",
    "        recon = model.inverse_transform(model.transform(X_tensor))\n",
    "        return np.array(model.get_components()), None, None, np.array(recon).T\n",
    "    results.append(benchmark(\"OjaPCA\", run_oja))\n",
    "\n",
    "    # fbpca (if available)\n",
    "    if HAS_FBPCA:\n",
    "        def run_fbpca():\n",
    "            U, s, Vt = fbpca.pca(X, k=p, raw=True)\n",
    "            X_recon = (U[:, :p] * s[:p]) @ Vt[:p, :]\n",
    "            return U, s, Vt, X_recon\n",
    "        results.append(benchmark(\"fBPCA\", run_fbpca))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]d:\\online_pca\\svd.py:263: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"svd.py\", line 96:\u001b[0m\n",
      "\u001b[1m@njit(parallel=True)\n",
      "\u001b[1mdef compute_score_cf_numba(i, j, x, d):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  new_vals[s] = compute_score_cf_numba(iq, s, x, d)[2]\n",
      "100%|██████████| 10000/10000 [00:51<00:00, 192.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial scores': {'total': 2.9325246999997034, 'count': 1, 'avg': 2.9325246999997034}, 'build heap': {'total': 0.3369719000002078, 'count': 1, 'avg': 0.3369719000002078}, 'perform small svd': {'total': 1.001683199991021, 'count': 10000, 'avg': 0.0001001683199991021}, 'perform matrix mul': {'total': 22.656983899934858, 'count': 10000, 'avg': 0.002265698389993486}, 'calculate row score': {'total': 3.5572049999100273, 'count': 10253, 'avg': 0.0003469428459875185}, 'rebuild heap': {'total': 3.9407712000393076, 'count': 10253, 'avg': 0.00038435298937279893}, 'update cols': {'total': 19.5495273998622, 'count': 10000, 'avg': 0.00195495273998622}, 'total time': {'total': 55.23344030000044, 'count': 1, 'avg': 55.23344030000044}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]d:\\online_pca\\svd.py:263: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"svd.py\", line 96:\u001b[0m\n",
      "\u001b[1m@njit(parallel=True)\n",
      "\u001b[1mdef compute_score_cf_numba(i, j, x, d):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  new_vals[s] = compute_score_cf_numba(iq, s, x, d)[2]\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 258.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial scores': {'total': 1.5277626999995846, 'count': 1, 'avg': 1.5277626999995846}, 'build heap': {'total': 0.11830000000009022, 'count': 1, 'avg': 0.11830000000009022}, 'perform small svd': {'total': 1.097901399996772, 'count': 10000, 'avg': 0.00010979013999967719}, 'perform matrix mul': {'total': 10.332526700016388, 'count': 10000, 'avg': 0.0010332526700016387}, 'calculate row score': {'total': 3.753846299883662, 'count': 10633, 'avg': 0.000353037364796733}, 'rebuild heap': {'total': 4.043215299820076, 'count': 10633, 'avg': 0.00038025160348162103}, 'update cols': {'total': 18.374999400059096, 'count': 10000, 'avg': 0.0018374999400059095}, 'total time': {'total': 40.321128400000816, 'count': 1, 'avg': 40.321128400000816}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 327/10000 [00:02<00:51, 188.32it/s]"
     ]
    }
   ],
   "source": [
    "X = load_mnist_and_fashion(n_samples=60000)\n",
    "# X = np.random.rand(784,300000).astype(np.float32)\n",
    "n_samples = 600000\n",
    "repeats = int(np.ceil(n_samples / X.shape[1]))\n",
    "X = np.tile(X, (1, repeats))[:, :n_samples]\n",
    "# np.random.seed(42) \n",
    "# X = np.random.rand(5, 8)\n",
    "p = 200\n",
    "g = 10000\n",
    "\n",
    "results = run_benchmarks(X, p=p, g=g)\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "for r in results:\n",
    "    print(f\"{r['method']:15s} | Time: {r['time']:.2f}s | Explained Var: {r['explained_variance']*100:.2f}%\")\n",
    "\n",
    "# Optional: bar plot\n",
    "methods = [r[\"method\"] for r in results]\n",
    "times = [r[\"time\"] for r in results]\n",
    "evrs = [r[\"explained_variance\"] for r in results]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.bar(methods, times, alpha=0.6, label=\"Time (s)\")\n",
    "ax2.plot(methods, evrs, \"o-\", color=\"red\", label=\"Explained Var\")\n",
    "\n",
    "ax1.set_ylabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Explained Variance\")\n",
    "plt.title(f\"PCA Benchmark (p={p}, g={g}) on MNIST subset\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online-pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
