{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d907e0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from oja_pca import OjaPCA\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from online_psp.online_psp.ccipca import CCIPCA\n",
    "from unified import fit_batched\n",
    "from scipy.sparse.linalg import svds\n",
    "from svd import ApproxSVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea448f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fbpca\n",
    "    HAS_FBPCA = True\n",
    "except ImportError:\n",
    "    HAS_FBPCA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c0a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_subset(n_samples=5000):\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X = mnist.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (784, n_samples)\n",
    "\n",
    "def load_fashion_mnist_subset(n_samples=5000):\n",
    "    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "    X = fmnist.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (784, n_samples)\n",
    "\n",
    "def load_usps_subset(n_samples=5000):\n",
    "    usps = fetch_openml('USPS', version=1, as_frame=False)\n",
    "    X = usps.data.astype(np.float32).T[:, :n_samples] / 255.0\n",
    "    return X  # shape: (256, n_samples) since USPS has 16x16 images\n",
    "\n",
    "def load_isolet_subset(n_samples=5000):\n",
    "    isolet = fetch_openml('isolet', version=1, as_frame=False)\n",
    "    # Features are already real-valued, just normalize by max\n",
    "    X = isolet.data.astype(np.float32).T[:, :n_samples]\n",
    "    X /= np.max(X)  # scale to [0,1]\n",
    "    return X  # shape: (617, n_samples), 617 audio features\n",
    "\n",
    "def load_mnist_and_fashion(n_samples=5000):\n",
    "    # Load MNIST\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X_mnist = mnist.data.astype(np.float32)[:n_samples] / 255.0\n",
    "    y_mnist = mnist.target.astype(int)[:n_samples]\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "    X_fmnist = fmnist.data.astype(np.float32)[:n_samples] / 255.0\n",
    "    y_fmnist = fmnist.target.astype(int)[:n_samples]\n",
    "\n",
    "    # Stack them together\n",
    "    X = np.vstack([X_mnist, X_fmnist]).T   # shape: (784, 2*n_samples)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a649f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance_ratio(X, X_recon):\n",
    "    error = np.linalg.norm(X - X_recon, 'fro') ** 2\n",
    "    print(X.shape)\n",
    "    print(X_recon.shape)\n",
    "    print(X[:3, :3])\n",
    "    print(X_recon[:3, :3])\n",
    "    total = np.linalg.norm(X, 'fro') ** 2\n",
    "    print(total)\n",
    "    return 1 - error / total\n",
    "\n",
    "def plot_traces(traces, X, p):\n",
    "    plt.plot(traces, label='cf', color = 'blue')\n",
    "    s_top, _, _ = svds(X, k=p)  \n",
    "    s_top = s_top[::-1]  # svds returns ascending order\n",
    "\n",
    "    # Total energy (Frobenius norm squared)\n",
    "    total_energy = np.linalg.norm(X, 'fro')**2\n",
    "\n",
    "    # Energy captured by top p components\n",
    "    true_energy = np.sum(s_top**2)\n",
    "    print(true_energy)\n",
    "\n",
    "    traces = np.array(traces)\n",
    "    u, s, vt = np.linalg.svd(X)\n",
    "    plt.plot(traces, label='cf', color = 'blue')\n",
    "    aux = u.transpose() @ X @ vt.transpose()\n",
    "    true_energy = np.trace(aux[:p, :p])\n",
    "\n",
    "\n",
    "    true_energy = np.sum(np.linalg.svd(X, compute_uv=False)[:p])\n",
    "\n",
    "    print(\"AB\")\n",
    "    print(true_energy)\n",
    "\n",
    "    print(traces[:-20])\n",
    "    plt.axhline(y=true_energy, color='red', linestyle='dotted', linewidth=2, label='Real U trace')\n",
    "    plt.ylim(bottom=min(0, traces.min()))\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Trace')\n",
    "    plt.title('Trace progression vs real U trace')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabe223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(method_name, fit_fn):\n",
    "    start = time.time()\n",
    "    U, S, Vt, X_recon = fit_fn()\n",
    "    elapsed = time.time() - start\n",
    "    evr = explained_variance_ratio(X, X_recon)\n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"time\": elapsed,\n",
    "        \"explained_variance\": evr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b3cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmarks(X, p=50, g=200):\n",
    "    results = []\n",
    "\n",
    "    #sklearn PCA (full SVD)\n",
    "    # def run_pca():\n",
    "    #     model = PCA(n_components=p, svd_solver=\"full\")\n",
    "    #     model.fit(X.T)\n",
    "    #     X_recon = model.inverse_transform(model.transform(X.T)).T\n",
    "    #     return model.components_.T, model.singular_values_, None, X_recon\n",
    "    # results.append(benchmark(\"PCA (full)\", run_pca))\n",
    "\n",
    "    # def run_approx_full_numba():\n",
    "    #     _, U, X_approx = fit_batched(n_iter = g,\n",
    "    #                                 p=p,\n",
    "    #                                 batch_size=1500,\n",
    "    #                                 trueX=X)\n",
    "        \n",
    "    #     X_reduced = U.T[:p, :] @ X\n",
    "    #     X_recon = U[:, :p] @ X_reduced\n",
    "    #     return U, None, None, X_recon\n",
    "    # results.append(benchmark(\"ApproxSVD_numba\", run_approx_full_numba))\n",
    "\n",
    "    #ApproxSVD\n",
    "    # def run_approx():\n",
    "    #     approx_svd = ApproxSVD(n_iter=g, p=p,\n",
    "    #                            score_method=\"cf\",\n",
    "    #                            debug_mode=False,\n",
    "    #                            jobs=8,\n",
    "    #                            stored_g = False,\n",
    "    #                            use_shared_memory=False,\n",
    "    #                            use_heap=\"optimized_heap\")\n",
    "    #     traces, U, X_approx = approx_svd.fit_batched(X, 3000)\n",
    "    #     X_reduced = U.T[:p, :] @ X\n",
    "    #     X_recon = U[:, :p] @ X_reduced\n",
    "    #     plot_traces(traces, X, p)\n",
    "    #     return U, None, None, X_recon\n",
    "    # results.append(benchmark(\"ApproxSVD\", run_approx))\n",
    "\n",
    "    def run_approx_unified():\n",
    "        traces, U, X_approx = fit_batched(X, p, g, 1500)\n",
    "        X_reduced = U.T[:p, :] @ X\n",
    "        X_recon = U[:, :p] @ X_reduced\n",
    "        plot_traces(traces, X, p)\n",
    "        return U, None, None, X_recon\n",
    "    results.append(benchmark(\"ApproxSVD-unified\", run_approx_unified))\n",
    "\n",
    "    # Incremental PCA\n",
    "    def run_incpca():\n",
    "        model = IncrementalPCA(n_components=p, batch_size=1500)\n",
    "        model.fit(X.T)\n",
    "        X_recon = model.inverse_transform(model.transform(X.T)).T\n",
    "        return model.components_.T, None, None, X_recon\n",
    "    results.append(benchmark(\"IncrementalPCA\", run_incpca))\n",
    "\n",
    "    #  #TruncatedSVD (randomized)\n",
    "    # def run_tsvd():\n",
    "    #     model = TruncatedSVD(n_components=p)\n",
    "    #     X_reduced = model.fit_transform(X.T)\n",
    "    #     # Reconstruction: approximate, since TSVD doesn't store mean\n",
    "    #     X_recon = (X_reduced @ model.components_).T\n",
    "    #     return model.components_.T, None, None, X_recon\n",
    "    # results.append(benchmark(\"TruncatedSVD\", run_tsvd))\n",
    "\n",
    "    # def run_oja():\n",
    "    #     model = OjaPCA(\n",
    "    #         n_features=X.shape[0],\n",
    "    #         n_components=p,\n",
    "    #         eta=0.005,\n",
    "    #     )\n",
    "    #     X_tensor = torch.tensor(X.T)\n",
    "    #     b_size = 1500\n",
    "    #     for i in range(0, len(X_tensor) - b_size, b_size):\n",
    "    #         batch = X_tensor[i : i + b_size]\n",
    "    #         if len(batch) < b_size:\n",
    "    #             # This line means we use up to an extra partial batch over 1 pass\n",
    "    #             batch = torch.cat([batch, X_tensor[: b_size - len(batch)]], dim=0)\n",
    "    #         error = model(batch) if hasattr(model, \"forward\") else None\n",
    "    #     recon = model.inverse_transform(model.transform(X_tensor))\n",
    "    #     return np.array(model.get_components()), None, None, np.array(recon).T\n",
    "    # results.append(benchmark(\"OjaPCA\", run_oja))\n",
    "\n",
    "    # def run_ccipca():\n",
    "    #     sigma2_0 = 1e-8 * np.ones(p)\n",
    "    #     Uhat0 = (X[:, :p] / np.sqrt((X[:, :p] ** 2).sum(0))).astype(np.float64)\n",
    "    #     ccipca = CCIPCA(p, X.shape[0], Uhat0=Uhat0, sigma2_0=sigma2_0, cython=True)\n",
    "    #     n_epoch = 2\n",
    "    #     for n_e in range(n_epoch):\n",
    "    #         for x in X.T:\n",
    "    #             ccipca.fit_next(x.astype(np.float64))\n",
    "    #     X_reduced = ccipca.get_components().T @ X\n",
    "    #     X_recon = ccipca.get_components() @ X_reduced\n",
    "    #     return np.array(ccipca.get_components()), None, None, np.array(X_recon)\n",
    "    # results.append(benchmark(\"CCIPCA\", run_ccipca))\n",
    "\n",
    "    # fbpca (if available)\n",
    "    if HAS_FBPCA:\n",
    "        def run_fbpca():\n",
    "            U, s, Vt = fbpca.pca(X, k=p, raw=True)\n",
    "            X_recon = (U[:, :p] * s[:p]) @ Vt[:p, :]\n",
    "            return U, s, Vt, X_recon\n",
    "        results.append(benchmark(\"fBPCA\", run_fbpca))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8396c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_mnist_subset(n_samples=15000)\n",
    "# X = np.random.rand(784,300000).astype(np.float32)\n",
    "n_samples = 15000\n",
    "\n",
    "# batch size should be 2 * d\n",
    "# ensure allocation of row copy only once\n",
    "# multiple maximums per row -> when updating a column\n",
    "# try to keep top-k maximums\n",
    "# check python profilers -> pycharm\n",
    "# settle for a \"smaller\" maximum\n",
    "# matrix mul should be faster -> look into it\n",
    "# see how matrices are kept -> row by row or column by column\n",
    "\n",
    "repeats = int(np.ceil(n_samples / X.shape[1]))\n",
    "X = np.tile(X, (1, repeats))[:, :n_samples]\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Generate a shuffled index for columns\n",
    "shuffled_indices = rng.permutation(n_samples)\n",
    "\n",
    "# Shuffle columns\n",
    "shuffled_arr = X[:, shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 1\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 2\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 3\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 4\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 5\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 6\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 7\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 8\n",
      "iteration 0\n",
      "iteration 3000\n",
      "iteration 6000\n",
      "iteration 9000\n",
      "iteration 12000\n",
      "iteration 15000\n",
      "iteration 18000\n",
      "Done batch 9\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# np.random.seed(42) \n",
    "# X = np.random.rand(5, 8)\n",
    "p = 50\n",
    "g = 20000\n",
    "\n",
    "results = run_benchmarks(X, p=p, g=g)\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "for r in results:\n",
    "    print(f\"{r['method']:15s} | Time: {r['time']:.2f}s | Explained Var: {r['explained_variance']*100:.2f}%\")\n",
    "\n",
    "# Optional: bar plot\n",
    "methods = [r[\"method\"] for r in results]\n",
    "times = [r[\"time\"] for r in results]\n",
    "evrs = [r[\"explained_variance\"] for r in results]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.bar(methods, times, alpha=0.6, label=\"Time (s)\")\n",
    "ax2.plot(methods, evrs, \"o-\", color=\"red\", label=\"Explained Var\")\n",
    "\n",
    "ax1.set_ylabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Explained Variance\")\n",
    "plt.title(f\"PCA Benchmark (p={p}, g={g}) on MNIST subset\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online-pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
